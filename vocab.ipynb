{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a79fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "from sklearn.manifold import TSNE\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from nltk.tokenize import word_tokenize ,sent_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdabc0e",
   "metadata": {},
   "source": [
    "### I. Import dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8499dcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 19:17:56: Reusing dataset cnn_dailymail (C:\\Users\\cheikh ahmed\\.cache\\huggingface\\datasets\\cnn_dailymail\\3.0.0\\3.0.0\\3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502e574d6fa8445bb9fdd3f4f1df5a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d786a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "val = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e395f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_train = train['article']\n",
    "resume_train = train['highlights']\n",
    "idi_train = train['id']\n",
    "\n",
    "\n",
    "article_val = val['article']\n",
    "resume_val = val['highlights']\n",
    "idi_val = val['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c7fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "article =article_train + article_val\n",
    "resume = resume_train + resume_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ec4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = article[:1000]\n",
    "resume = resume[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e677788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 200000\n",
    "\n",
    "class Makedata:\n",
    "        \n",
    "    def concacate(self , article , resume):\n",
    "        article = [art.lower() for art in article]\n",
    "        resume =[art.lower() for art in resume]\n",
    "        #for i in tqdm(range(len(article))):\n",
    "            #art = art + article[i] + resume[i]\n",
    "        a = article + resume\n",
    "        return ' '.join(a)\n",
    "\n",
    "    def createVocab(self , article  ,resume, size , filename):\n",
    "        \n",
    "        file = open(filename,\"a\",encoding=\"utf-8\")\n",
    "        word_freq = Counter(word_tokenize(self.concacate(article , resume)))\n",
    "        top_k_words = sorted(word_freq.keys(), reverse=True, key=word_freq.get)[:size]\n",
    "        count = 0\n",
    "        for word in top_k_words:\n",
    "            if count < size:\n",
    "                if not (word_freq[word] == 1):\n",
    "                    file.write(word+\" \\n\")\n",
    "                    count+=1\n",
    "        return \n",
    "        \n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, vocab_file, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\" ,2 : \"UNK\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "        self.oovs = []\n",
    "        \n",
    "        \n",
    "        with open(vocab_file, 'r' ,encoding=\"utf-8\") as vocab_f:\n",
    "              for line in vocab_f:\n",
    "                pieces = line\n",
    "                w = pieces.split(\" \")[0]\n",
    "                if w in [\"UNK\", \"SOS\" , \"EOS\"]:\n",
    "                    raise Exception('[UNK], [SOS] and [EOS] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "                if w in self.word2index:\n",
    "                    raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "                self.word2index[w] = self.n_words\n",
    "                self.index2word[self.n_words] = w\n",
    "                self.n_words += 1\n",
    "                if vocab_size != 0 and self.n_words >= vocab_size:\n",
    "                     break\n",
    "            \n",
    "    def ArticleToindex(self , article):\n",
    "        ids =[]\n",
    "        for word in word_tokenize(article.lower()):\n",
    "            if(word not in self.word2index ):\n",
    "                if word not in self.oovs:\n",
    "                    self.oovs.append(word)\n",
    "                oov_num = self.oovs.index(word)\n",
    "                ids.append(self.n_words + oov_num)\n",
    "            else:\n",
    "                ids.append(self.word2index[word])\n",
    "        return ids\n",
    "    \n",
    "    def resumeToindex(self, resume):\n",
    "            ids = []\n",
    "            for word in word_tokenize(resume.lower()):\n",
    "                if(word not in self.word2index ):\n",
    "                    if word in self.oovs: # If w is an in-article OOV\n",
    "                            vocab_idx = self.n_words + self.oovs.index(word) # Map to its temporary article OOV number\n",
    "                            ids.append(vocab_idx)\n",
    "                    else: # If w is an out-of-article OOV\n",
    "                        ids.append(2) # Map to the UNK token id\n",
    "                else:\n",
    "                     ids.append(self.word2index[word])\n",
    "            return ids\n",
    "    \n",
    "    def genrate_input(self , article , resume):\n",
    "        input=[self.ArticleToindex(art) for art in article]\n",
    "        resume_out = [self.resumeToindex(res) for res in resume]\n",
    "        leng_max = max([len(art) for art in input])\n",
    "        len_max = max([len(res) for res in resume_out])\n",
    "        input = pad_sequences(input, padding='post' ,maxlen=leng_max) \n",
    "        resume_out = pad_sequences(resume_out, padding='post' ,maxlen=len_max) \n",
    "        return input , resume_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3d870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
